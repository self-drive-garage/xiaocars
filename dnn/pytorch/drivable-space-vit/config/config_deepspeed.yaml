# DeepSpeed Configuration - Optimized for 16 GPUs with 16GB each using ZeRO-3

model:
  # Basic parameters
  img_size: 256
  patch_size: 16
  in_chans: 3
  embed_dim: 768
  num_heads: 12
  mlp_ratio: 4
  dropout: 0.1
  attn_dropout: 0.1
  ego_motion_dim: 9
  
  # Modular architecture parameters
  spatial_layers: 4
  cross_view_layers: 4
  temporal_layers: 4

dataset:
  data_dir: "datasets/argoversev2"
  seq_len: 3
  batch_size: 2
  num_workers: 4
  random_sequence: false
  cache_images: false
  temporal:
    stride: 1
    overlap: 0
    max_gap: 0.3
    log_handling:
      min_log_frames: 10
      max_log_frames: 1000
    validation:
      stride: 5
      max_gap: 0.3

training:
  resume: "outputs/deepspeed/checkpoint_epoch_*"  # Enable resume - will use latest checkpoint if exists
  lr: 8e-4
  warmup_epochs: 15
  min_lr: 1e-5
  lr_schedule: "cosine"
  
  optimizer: "AdamW"
  weight_decay: 1e-4
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8

  temporal_weight: 1.0
  reconstruction_weight: 1.0
  consistency_weight: 1.0
  future_weight: 1.0
  
  # DeepSpeed specific parameters
  gradient_accumulation: 8
  gradient_clipping: 1.0
  
  # DeepSpeed ZeRO configuration
  zero_optimization:
    stage: 3  # ZeRO-3 for parameter sharding
    cpu_offload: false  # Keep on GPU for better performance
    overlap_comm: true
    contiguous_gradients: true
    bucket_size: 5e8
    stage3_max_live_parameters: 1e9
    stage3_max_reuse_distance: 1e9
    stage3_prefetch_bucket_size: 5e7
    stage3_param_persistence_threshold: 1e6
    gather_16bit_weights_on_model_save: true
    round_robin_gradients: true
  
  # Mixed precision settings for DeepSpeed
  fp16:
    enabled: true
    loss_scale: 0
    initial_scale_power: 16
    loss_scale_window: 1000
    hysteresis: 2
    min_loss_scale: 1
  
  # Communication backend
  comms_logger:
    enabled: false
  
  # Memory optimizations
  activation_checkpointing:
    partition_activations: true
    cpu_checkpointing: false
    contiguous_memory_optimization: true
    synchronize_checkpoint_boundary: true
  
  # Debugging options
  debug: true  # Enable model inspection and detailed diagnostics
  
  epochs: 300

logging:
  output_dir: "outputs/deepspeed"
  log_interval: 10
  save_interval: 5
  eval_interval: 1
  visualize_every: 1000  # Temporarily disabled by setting to very large value
  num_viz_samples: 5
  visualize_sequences: true
  
  profiling: 
    enabled: true
    start_step: 100
    end_step: 110
    output_dir: "profiles/deepspeed"

# DeepSpeed JSON configuration will be generated from this
deepspeed:
  train_batch_size: 256  # 2 * 8 accumulation * 16 GPUs
  train_micro_batch_size_per_gpu: 2
  gradient_accumulation_steps: 8
  wall_clock_breakdown: false
  dump_state: false

hydra:
  job:
    chdir: false 