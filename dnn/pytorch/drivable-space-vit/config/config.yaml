# Drivable Space ViT Configuration

# Model architecture parameters
model:
  img_size: 256
  patch_size: 16  
  num_channels: 3
  embed_dim: 512
  hidden_dim: 128
  num_heads: 8
  num_layers: 12
  mlp_ratio: 4
  dropout: 0.1
  attn_dropout: 0.1
  ego_motion_dim: 9

# Dataset parameters
dataset:
  seq_len: 3              # Number of frames used by the model for one sample
  batch_size: 8           # Reduced from 8 to save memory
  num_workers: 8          # Number of worker processes for data loading
  random_sequence: true   # Enable random sequence sampling for training
  cache_images: false     # Whether to cache images in memory
  temporal:
    stride: 1             # Number of frames to skip between sequences
    overlap: 0            # Number of overlapping frames between sequences
    max_gap: 0.15         # Maximum allowed time gap between consecutive frames (in seconds)
                          # 0.15s allows for some variation from the expected 0.1s (10Hz)
    log_handling:
      min_log_frames: 10  # Minimum frames required in a log
      max_log_frames: 1000  # Maximum frames to use from a log
    validation:
      stride: 5           # Larger stride for validation
      max_gap: 0.12       # Stricter gap requirement for validation (closer to 10Hz)

training:
  # Learning rate and schedule parameters
  lr: 3e-4                     # Increased from 1e-4 for faster convergence
  warmup_epochs: 5             # Reduced from 10 for more efficient training
  min_lr: 1e-5                 # Increased from 1e-6 to prevent stalling
  lr_schedule: "cosine"        # Confirm using cosine schedule
  
  # Optimizer parameters
  optimizer: "AdamW"           # Explicitly use AdamW instead of Adam
  weight_decay: 1e-4           # Keep your current weight decay
  beta1: 0.9                   # Adam/AdamW beta1 parameter (add this)
  beta2: 0.999                 # Adam/AdamW beta2 parameter (add this)
  eps: 1e-8                    # Adam/AdamW epsilon parameter (add this)
  
  # Loss weights - adjusted based on your training logs
  reconstruction_weight: 1.0   # Keep reconstruction weight
  consistency_weight: 0.5      # Reduce from 1.0 since this component is already well-optimized
  future_weight: 1.0           # Increase from 0.5 to focus more on future prediction
  temporal_weight: 0.5         # Increase from 0.3 to enhance temporal consistency
  
  # Training optimizations
  mixed_precision: true        # Keep mixed precision for memory efficiency
  gradient_accumulation: 8     # Keep existing accumulation steps
  gradient_clipping: 1.0       # Keep gradient clipping to prevent explosions
  
  # Additional parameters to consider
  epochs: 300                  # Keep your planned 300 epochs
  amp_device: "cuda"           # Specify device for automatic mixed precision (to fix warnings)

# Logging and saving parameters
logging:
  log_interval: 10        # Logging interval in steps
  save_interval: 5        # Checkpoint saving interval in epochs
  eval_interval: 1        # Evaluation interval in epochs
  visualize_every: 5      # Visualize predictions every N epochs
  num_viz_samples: 5     # Number of samples to visualize
  visualize_sequences: true  # New: Whether to visualize temporal sequences 
  