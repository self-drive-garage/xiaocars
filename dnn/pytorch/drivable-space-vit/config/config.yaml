# Drivable Space ViT Configuration - Optimized for Hybrid Parallelism (DP=4, PP=2, TP=2)

# Model architecture parameters
model:
  img_size: 320           # Increased from 256 to benefit from distributed computation
  patch_size: 16          # Keep patch size consistent for now
  num_channels: 3         # Keep RGB input
  embed_dim: 1024         # Increased from 512 to 1024 (2x larger, matching TP=2)
  hidden_dim: 256         # Increased from 128 to 256 (2x larger, matching TP=2)
  num_heads: 16           # Increased from 8 to 16 (must be divisible by TP=2)
  num_layers: 24          # Increased from 12 to 24 (2x larger, matching PP=2)
  mlp_ratio: 4            # Keep expansion ratio
  dropout: 0.1            # Keep dropout rate
  attn_dropout: 0.1       # Keep attention dropout
  ego_motion_dim: 9       # Keep ego motion dimensions

# Dataset parameters
dataset:
  seq_len: 5              # Increased from 3 to 5 for better temporal modeling
  batch_size: 4           # Reduced per-GPU batch size (effective batch = 4 * DP_size * grad_accum = 4 * 4 * 8 = 128)
  num_workers: 4          # Slightly reduced for stability across many GPUs
  random_sequence: true   # Keep random sequence sampling
  cache_images: false     # Keep images uncached (with distributed processing, caching gives less benefit)
  temporal:
    stride: 1             # Keep temporal stride
    overlap: 0            # Keep no overlap
    max_gap: 0.15         # Keep max gap between frames
    log_handling:
      min_log_frames: 10  # Keep min frames per log
      max_log_frames: 1000 # Keep max frames per log
    validation:
      stride: 5           # Keep validation stride
      max_gap: 0.12       # Keep validation max gap

training:
  # Learning rate and schedule parameters
  lr: 5e-4                     # Increased for larger model/batch size (scales with batch size)
  warmup_epochs: 10            # Increased back to 10 for larger model stability
  min_lr: 1e-5                 # Keep min learning rate
  lr_schedule: "cosine"        # Keep cosine schedule
  
  # Optimizer parameters
  optimizer: "AdamW"           # Keep AdamW
  weight_decay: 1e-4           # Keep weight decay
  beta1: 0.9                   # Keep beta1
  beta2: 0.999                 # Keep beta2
  eps: 1e-8                    # Keep epsilon
  
  # Loss weights - adjusted for larger model
  reconstruction_weight: 1.0   # Keep reconstruction weight
  consistency_weight: 0.5      # Keep reduced consistency weight
  future_weight: 1.0           # Keep increased future weight
  temporal_weight: 0.5         # Keep temporal weight
  
  # Training optimizations
  mixed_precision: true        # Keep mixed precision for memory efficiency
  gradient_accumulation: 8     # Keep gradient accumulation steps
  gradient_clipping: 1.0       # Keep gradient clipping
  
  # Add ZeRO optimization parameters
  zero_stage: 2                # ZeRO stage 2 (optimizer state sharding)
  zero_cpu_offload: true       # Enable CPU offloading for optimizer states
  zero_param_offload: false    # No parameter offloading yet (enable if OOM)
  
  # Add pipeline parallelism parameters
  pipe_chunks: 2               # Micro-batches per pipeline (keep low initially)
  pipe_interleaved: true       # Use interleaved scheduling for better GPU utilization
  
  # Additional parameters
  epochs: 300                  # Keep 300 epochs
  amp_device: "cuda"           # Keep device specification

# DeepSpeed specific parameters
deepspeed:
  activation_checkpointing: true    # Enable activation checkpointing for memory savings
  cpu_checkpointing: false          # Only enable if struggling with GPU memory
  contiguous_memory_optimization: true # Enable memory optimization
  synchronize_checkpoint_boundary: true # Synchronize on checkpoint boundaries

# Logging and saving parameters
logging:
  log_interval: 10        # Keep logging interval
  save_interval: 5        # Keep checkpoint saving interval
  eval_interval: 1        # Keep evaluation interval
  visualize_every: 5      # Keep visualization frequency
  num_viz_samples: 5      # Keep number of samples to visualize
  visualize_sequences: true # Keep sequence visualization
  
  # Add new parameters for distributed training
  profiling: 
    enabled: true          # Enable profiling
    start_step: 100        # Start profiling after warmup
    end_step: 110          # Profile for 10 steps
    output_dir: "profiles" # Directory for profiling outputs
