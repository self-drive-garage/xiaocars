# Native PyTorch FSDP Configuration - Optimized for 16 GPUs with 16GB each

model:
  # Basic parameters
  img_size: 256
  patch_size: 16
  in_chans: 3
  embed_dim: 768
  num_heads: 12
  mlp_ratio: 4
  dropout: 0.1
  attn_dropout: 0.1
  ego_motion_dim: 9
  
  # Modular architecture parameters
  spatial_layers: 4
  cross_view_layers: 4
  temporal_layers: 4

dataset:
  data_dir: "datasets/argoversev2"
  seq_len: 3
  batch_size: 2
  num_workers: 4
  random_sequence: false
  cache_images: false
  temporal:
    stride: 1
    overlap: 0
    max_gap: 0.3
    log_handling:
      min_log_frames: 10
      max_log_frames: 1000
    validation:
      stride: 5
      max_gap: 0.3

training:
  # resume: "outputs/pytorch_fsdp/checkpoint_epoch_100.pth"
  lr: 8e-4
  warmup_epochs: 15
  min_lr: 1e-5
  lr_schedule: "cosine"
  
  optimizer: "AdamW"
  weight_decay: 1e-4
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  
  reconstruction_weight: 1.0
  consistency_weight: 0.5
  future_weight: 1.0
  temporal_weight: 0.5
  
  mixed_precision: false
  gradient_accumulation: 8
  gradient_clipping: 1.0
  
  # Native PyTorch FSDP parameters
  fsdp_sharding_strategy: "FULL_SHARD"  # Equivalent to ZeRO-3
  fsdp_cpu_offload: false
  fsdp_backward_prefetch: "BACKWARD_PRE"
  fsdp_use_orig_params: true
  fsdp_state_dict_type: "SHARDED_STATE_DICT"
  
  # Memory optimizations
  activation_checkpointing: true
  
  # Debugging options
  debug: true  # Enable model inspection and detailed diagnostics
  
  epochs: 300
  amp_device: "cuda"

logging:
  output_dir: "outputs/pytorch_fsdp"
  log_interval: 10
  save_interval: 5
  eval_interval: 1
  visualize_every: 5
  num_viz_samples: 5
  visualize_sequences: true
  
  profiling: 
    enabled: true
    start_step: 100
    end_step: 110
    output_dir: "profiles/pytorch_fsdp" 
hydra:
  job:
    chdir: false